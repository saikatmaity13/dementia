## ---------------------------------
##  EDA FOR DEMENTIA TRANSCRIPTS
## ---------------------------------
#
# This script performs Exploratory Data Analysis on the two files
# generated by the previous steps:
#
# 1. transcripts_classifier_clean.csv: For analyzing text features.
# 2. transcripts.csv: For analyzing metadata (speakers, tasks).
#
# Make sure you have the required libraries installed:
# pip install pandas matplotlib seaborn wordcloud nltk
#
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from sklearn.feature_extraction.text import CountVectorizer
import sys
import os

# --- NLTK setup (for tokenizing in this script) ---
try:
    import nltk
    nltk.download('punkt', quiet=True)
except ImportError:
    print("NLTK not found. Please install: pip install nltk")
    sys.exit()

# --- Configuration ---
CLEAN_DATA_FILE = 'transcripts_classifier_clean.csv'
RAW_DATA_FILE = 'transcripts.csv'
EDA_OUTPUT_FOLDER = 'eda_plots'

# Create a folder to save the plots
if not os.path.exists(EDA_OUTPUT_FOLDER):
    os.makedirs(EDA_OUTPUT_FOLDER)
    print(f"Created folder: {EDA_OUTPUT_FOLDER}")
    
# Set plot style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 7)

# ===================================================================
## PART 1: ANALYSIS OF CLEANED TEXT (transcripts_classifier_clean.csv)
# ===================================================================
print("\n--- PART 1: Analyzing Cleaned Text Features ---")

try:
    df_clean = pd.read_csv(CLEAN_DATA_FILE)
except FileNotFoundError:
    print(f"Error: '{CLEAN_DATA_FILE}' not found.")
    print("Please run 'clean_data.py' first.")
    sys.exit()

# --- 1.1: Basic Info & Target Variable Distribution ---
print(f"\nLoaded '{CLEAN_DATA_FILE}' with {len(df_clean)} records.")
print("\nData Head:")
print(df_clean.head())

print("\nChecking for missing values:")
print(df_clean.isnull().sum())

# Plot Target Variable Distribution
print("\nPlotting Target Variable (Diagnosis) Distribution...")
plt.figure(figsize=(8, 5))
ax = sns.countplot(x='Diagnosis', data=df_clean, palette="viridis")
ax.set_title("Class Distribution (dementia vs. nondementia)", fontsize=16)
plt.xlabel("Diagnosis")
plt.ylabel("Count")
# Add counts on top of bars
for p in ax.patches:
   ax.annotate(f'{p.get_height()}', 
               (p.get_x() + p.get_width() / 2., p.get_height()), 
               ha='center', va='center', 
               xytext=(0, 9), 
               textcoords='offset points')
plt.savefig(os.path.join(EDA_OUTPUT_FOLDER, '1_class_distribution.png'))
plt.show()

if df_clean['Diagnosis'].nunique() > 2:
    print(f"Warning: Found more than 2 classes: {df_clean['Diagnosis'].unique()}")


# --- 1.2: Text Statistics (Word and Character Counts) ---
print("Calculating text statistics (word/char counts)...")

# Handle potential NaN values from cleaning
df_clean['Processed_Text'] = df_clean['Processed_Text'].fillna('')

df_clean['word_count'] = df_clean['Processed_Text'].apply(lambda x: len(x.split()))
df_clean['char_count'] = df_clean['Processed_Text'].apply(len)

# Plot Word Count Distribution
plt.figure(figsize=(12, 6))
sns.histplot(data=df_clean, x='word_count', hue='Diagnosis', kde=True, palette="mako")
plt.title("Distribution of Word Count (Cleaned Text) by Diagnosis", fontsize=16)
plt.xlabel("Word Count")
plt.ylabel("Frequency")
plt.savefig(os.path.join(EDA_OUTPUT_FOLDER, '2_word_count_distribution.png'))
plt.show()

# Plot Character Count Distribution
plt.figure(figsize=(12, 6))
sns.histplot(data=df_clean, x='char_count', hue='Diagnosis', kde=True, palette="flare")
plt.title("Distribution of Character Count (Cleaned Text) by Diagnosis", fontsize=16)
plt.xlabel("Character Count")
plt.ylabel("Frequency")
plt.savefig(os.path.join(EDA_OUTPUT_FOLDER, '3_char_count_distribution.png'))
plt.show()

# --- 1.3: Word Cloud Visualization ---
print("Generating word clouds...")

# Combine all text for a general word cloud
all_text = ' '.join(df_clean['Processed_Text'])
wordcloud_all = WordCloud(width=800, height=400, background_color='white').generate(all_text)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_all, interpolation='bilinear')
plt.title("Word Cloud - All Transcripts", fontsize=16)
plt.axis('off')
plt.savefig(os.path.join(EDA_OUTPUT_FOLDER, '4_wordcloud_all.png'))
plt.show()

# Separate word clouds for each class
try:
    text_dementia = ' '.join(df_clean[df_clean['Diagnosis'] == 'dementia']['Processed_Text'])
    text_nondementia = ' '.join(df_clean[df_clean['Diagnosis'] == 'nondementia']['Processed_Text'])

    if text_dementia:
        wc_dementia = WordCloud(width=800, height=400, background_color='white').generate(text_dementia)
        plt.figure(figsize=(10, 5))
        plt.imshow(wc_dementia, interpolation='bilinear')
        plt.title("Word Cloud - 'dementia' Class", fontsize=16)
        plt.axis('off')
        plt.savefig(os.path.join(EDA_OUTPUT_FOLDER, '5_wordcloud_dementia.png'))
        plt.show()
    else:
        print("No text found for 'dementia' class word cloud.")
        
    if text_nondementia:
        wc_nondementia = WordCloud(width=800, height=400, background_color='white').generate(text_nondementia)
        plt.figure(figsize=(10, 5))
        plt.imshow(wc_nondementia, interpolation='bilinear')
        plt.title("Word Cloud - 'nondementia' Class", fontsize=16)
        plt.axis('off')
        plt.savefig(os.path.join(EDA_OUTPUT_FOLDER, '6_wordcloud_nondementia.png'))
        plt.show()
    else:
        print("No text found for 'nondementia' class word cloud.")
        
except KeyError:
    print("Warning: Could not find 'dementia' or 'nondementia' labels. Skipping class-based word clouds.")
except Exception as e:
    print(f"An error occurred generating word clouds: {e}")


# --- 1.4: N-gram Analysis (Top Bigrams and Trigrams) ---
print("Calculating top N-grams...")

def get_top_ngrams(corpus, n_gram, top_n):
    """Helper function to get top n-grams."""
    vec = CountVectorizer(ngram_range=(n_gram, n_gram), stop_words='english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)
    return words_freq[:top_n]

def plot_top_ngrams(df_freq, title, filename):
    """Helper function to plot n-grams."""
    plt.figure(figsize=(12, 8))
    sns.barplot(x='count', y='ngram', data=df_freq, palette='viridis')
    plt.title(title, fontsize=16)
    plt.xlabel("Count")
    plt.ylabel("N-gram")
    plt.tight_layout()
    plt.savefig(os.path.join(EDA_OUTPUT_FOLDER, filename))
    plt.show()

# Top 20 Bigrams for each class
try:
    top_bigrams_dementia = get_top_ngrams(df_clean[df_clean['Diagnosis'] == 'dementia']['Processed_Text'], 2, 20)
    df_bigrams_dementia = pd.DataFrame(top_bigrams_dementia, columns=['ngram', 'count'])
    plot_top_ngrams(df_bigrams_dementia, "Top 20 Bigrams - 'dementia' Class", '7_bigrams_dementia.png')

    top_bigrams_nondementia = get_top_ngrams(df_clean[df_clean['Diagnosis'] == 'nondementia']['Processed_Text'], 2, 20)
    df_bigrams_nondementia = pd.DataFrame(top_bigrams_nondementia, columns=['ngram', 'count'])
    plot_top_ngrams(df_bigrams_nondementia, "Top 20 Bigrams - 'nondementia' Class", '8_bigrams_nondementia.png')
except Exception as e:
    print(f"Could not generate n-gram plots: {e}")


# ===================================================================
## PART 2: ANALYSIS OF METADATA (transcripts.csv)
# ===================================================================
print("\n--- PART 2: Analyzing Raw Metadata (Checking for leaks) ---")

try:
    df_raw = pd.read_csv(RAW_DATA_FILE)
except FileNotFoundError:
    print(f"Error: '{RAW_DATA_FILE}' not found.")
    print("This file is needed for metadata analysis. Skipping Part 2.")
    sys.exit()

print(f"\nLoaded '{RAW_DATA_FILE}' with {len(df_raw)} records.")

# --- 2.1: Speaker Analysis ---
speaker_counts = df_raw['Speaker_ID'].nunique()
print(f"\nTotal unique speakers: {speaker_counts}")

# Check for speaker overlap (e.g., if one speaker is in both classes)
speaker_overlap = df_raw.groupby('Speaker_ID')['Diagnosis'].nunique()
speakers_in_multiple_classes = speaker_overlap[speaker_overlap > 1]

if len(speakers_in_multiple_classes) > 0:
    print("\n!! CRITICAL WARNING: DATA LEAK DETECTED !!")
    print("The following speakers appear in MORE THAN ONE diagnosis class:")
    print(speakers_in_multiple_classes)
else:
    print("\nGOOD: No speakers appear in more than one diagnosis class.")

# Plot transcripts per speaker
plt.figure(figsize=(10, 5))
sns.countplot(x='Diagnosis', data=df_raw, hue='Speaker_ID', palette='Set3')
plt.title("Transcripts per Speaker, Grouped by Diagnosis")
plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='Speaker_ID')
plt.tight_layout()
plt.savefig(os.path.join(EDA_OUTPUT_FOLDER, '9_transcripts_per_speaker.png'))
plt.show()

# --- 2.2: Task ID Analysis ---
task_counts = df_raw['Task_ID'].nunique()
print(f"\nTotal unique tasks: {task_counts} (Tasks: {df_raw['Task_ID'].unique()})")

# Plot distribution of Task_ID
plt.figure(figsize=(10, 6))
sns.countplot(x='Task_ID', data=df_raw, palette="Set2")
plt.title("Distribution of Task IDs (All Data)", fontsize=16)
plt.savefig(os.path.join(EDA_OUTPUT_FOLDER, '10_task_id_distribution.png'))
plt.show()

# Check for task bias (e.g., if 'Task_5' is only for dementia patients)
print("\nChecking for Task ID bias by Diagnosis (Crosstab):")
task_crosstab = pd.crosstab(df_raw['Task_ID'], df_raw['Diagnosis'])
print(task_crosstab)

# Plot the crosstab
task_crosstab.plot(kind='bar', stacked=True, figsize=(12, 7), colormap='viridis')
plt.title("Task ID Distribution by Diagnosis Class", fontsize=16)
plt.ylabel("Count")
plt.xlabel("Task ID")
plt.xticks(rotation=0)
plt.savefig(os.path.join(EDA_OUTPUT_FOLDER, '11_task_id_by_diagnosis.png'))
plt.show()

print("\n--- EDA Complete ---")
print(f"All plots have been saved to the '{EDA_OUTPUT_FOLDER}' folder.")
print("Review the plots and console output for insights.")